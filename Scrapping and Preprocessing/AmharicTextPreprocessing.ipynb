{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Sintayehu Zekarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to preprocess the file\n",
    "##### 1. Open Each categroy \n",
    "##### 2. Preprocess_sentence_by_sentence for each category and save the prepreocessed file based on the category name into SentenceBasedPreprocessedCategory folder\n",
    "     2.1.  remove_punc_and_special_chars(contentFile )\n",
    "     2.2.  remove_non_amharic_ascii_and_numbers(text2 )\n",
    "     2.3.  normalize_char_level_missmatch(text2 )                       \n",
    "     2.4.  fileAppend.write(text2) \n",
    "##### 3. tokenize word from sentence and removing duplicate word from list\n",
    "     3.1. Tokenize word form sentence\n",
    "\n",
    "        tokenized_word_list= tokenize_word_from_sentence()\n",
    "        Before removed duplicated word: 5802093\n",
    "        \n",
    "        \n",
    "     3.2. Remove duplcate word from tokenized_word_list\n",
    "          final_list , word_list = remove_duplcate(tokenized_word_list)\n",
    "          After removed duplicated word: 447137\n",
    "\n",
    "##### 4. Remove Prefix from word_list to make it ready for segmntation\n",
    "     result,  word_new_list, prefix_segmented_list = prefix_segemntation(word_list)\n",
    "##### 5. Add list of prefix and suffix words into word_new_list \n",
    "     word_new_list, prefix_segmented_list=add_list_of_prefix_suffix_on_words(prefix_segmented_list,word_new_list)\n",
    "##### 6. Save the word list into WordTokenized folder as \"words\"\n",
    "     write_list_of_words(word_new_list)\n",
    "So \"words\" are ready to be morhological processing , \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations like . , ! $( ) * % @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing any existance of special character or punctuation to null  \n",
    "def remove_punc_and_special_chars(sentence_input): # puct in amh =፡።፤;፦፧፨፠፣ \n",
    "    normalized_text = re.sub('[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\_,\\•,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣\\‑\\–\\—\\„\\†\\‰\\€\\−\\∙\\☎]', '',sentence_input) \n",
    "    return normalized_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non Amharic words , all ascii characters and Arabic and Amharic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_amharic_ascii_and_numbers(text_input):\n",
    "    regex=re.sub('[A-Za-z0-9]','',text_input)\n",
    "    return re.sub('[\\'\\u1369-\\u137C\\']+','',regex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize character level missmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to normalize character level missmatch such as ጸሀይ and ፀሐይ\n",
    "def normalize_char_level_missmatch(input_token):\n",
    "    rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',input_token)\n",
    "    rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "    rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "    rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "    rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "    rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "    rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "    rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "    rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "    rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "    rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "    rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "    rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "    rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "    rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "    rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "    rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "    rep18=re.sub('[ዕ]','እ',rep17)\n",
    "    rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "    rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "    rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "    rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "    rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "    rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "    rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "    rep26=re.sub('[ጾ]','ፆ',rep25)\n",
    "    #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል  \n",
    "    rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "    rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "    rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "    rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "    rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "    rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "    rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "    rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "    rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "    rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "    rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "    rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "    rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "    rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "    rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "    rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "    rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "    rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "    rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "    rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "    rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "    rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ  \n",
    "    \n",
    "    return rep48"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_tokenize(corpus):\n",
    "#     # print('Word Tokenization ...')\n",
    "#     all_tokens=[]\n",
    "\n",
    "#     for sentence in corpus:\n",
    "#         tokens=sentence.split() # expecting non-sentence identifies are already removed\n",
    "#         all_tokens.extend(tokens)\n",
    "\n",
    "#     return all_tokens\n",
    "\n",
    "def word_tokenize(corpus):\n",
    "    # print('Word Tokenization ...')  \n",
    "    tokens=corpus.split() # expecting non-sentence identifies are already removed    \n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(corpus):\n",
    "    # print('Sentence Tokenization ...')\n",
    "    \n",
    "    sentence = re.compile('[!?።(\\፡\\፡)]+').split(corpus)\n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence by sentence, preprocess the scraped file, then store it according to the category name- Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_by_sentence():\n",
    "\n",
    "    #Initialization\n",
    "    path = 'Category'\n",
    "    myCategoryList = os.listdir(path)\n",
    "    myCategoryList\n",
    "\n",
    "    # Parent Directories \n",
    "    parent_dir = os.getcwd() \n",
    "    result = \"\"\n",
    "\n",
    "    for category in myCategoryList:\n",
    "        path =  parent_dir + \"\\\\\" + \"SentenceBasedPreprocessdCategory\"\n",
    "        filePath = os.path.join(path, category + \".txt\") \n",
    "        # Create the directory \n",
    "        isExist = os.path.exists(filePath)\n",
    "        # if not isExist:\n",
    "        file = open(filePath, \"w\" , encoding='utf-8') \n",
    "        # print(result)\n",
    "        file.write(result)\n",
    "\n",
    "        file.flush()\n",
    "        file.close\n",
    "\n",
    "        path =  parent_dir +  \"\\\\Category\\\\\" + category\n",
    "        myFileList = os.listdir(path)\n",
    "        for fileName in myFileList:\n",
    "            completeName = os.path.join(path, fileName) \n",
    "            with open(completeName, 'r', encoding='utf-8') as fileRead:\n",
    "                text2 = fileRead.read()\n",
    "\n",
    "                with open(filePath, \"a\", encoding='utf-8') as fileAppend:\n",
    "                \n",
    "                    text2=sentence_tokenize(text2)\n",
    "                \n",
    "                    contentFile= ' '.join(text2)\n",
    "                    text2=remove_punc_and_special_chars(contentFile )\n",
    "                    text2=remove_non_amharic_ascii_and_numbers(text2 )\n",
    "                    text2=normalize_char_level_missmatch(text2 )\n",
    "                    #print(' '.join(text2))\n",
    "                    fileAppend.write(text2)      \n",
    "                    fileAppend.flush()\n",
    "                    fileAppend.close\n",
    "                fileRead.flush()\n",
    "                fileRead.close\n",
    "       \n",
    "       \n",
    "preprocess_sentence_by_sentence()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the content of a sentence-based preprocessed file - Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word_from_sentence():\n",
    "\n",
    "    #Initialization\n",
    "    path = 'SentenceBasedPreprocessdCategory'\n",
    "    myFileList = os.listdir(path)\n",
    "    \n",
    "\n",
    "    tokenized_word_list =[]\n",
    "    # Parent Directories \n",
    "    parent_dir = os.getcwd()  + \"\\\\\" + \"SentenceBasedPreprocessdCategory\"\n",
    "   \n",
    "\n",
    "    for category in myFileList:\n",
    "        completeName = os.path.join(parent_dir, category) \n",
    "        with open(completeName, 'r', encoding='utf-8') as fileRead:\n",
    "            file_contents = fileRead.read()\n",
    "            tokenized_word =  word_tokenize(file_contents)\n",
    "            for word in tokenized_word:\n",
    "               \n",
    "                tokenized_word_list.append(word)\n",
    "            fileRead.flush()\n",
    "            fileRead.close\n",
    "    return tokenized_word_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate word from tokenized word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplcate(tokenized_word_list):\n",
    "    final_list = \"\"\n",
    "    word_list = []\n",
    "    for word in tokenized_word_list:\n",
    "        if word not in final_list:\n",
    "            final_list += word + \"\\n\"\n",
    "            word_list.append(word)\n",
    "    return final_list, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removed duplicated word: 5800192\n",
      "After removed duplicated word: 447083\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_list= tokenize_word_from_sentence()\n",
    "\n",
    "print(\"Before removed duplicated word:\",len(tokenized_word_list))\n",
    "\n",
    "\n",
    "final_list , word_list = remove_duplcate(tokenized_word_list)\n",
    "\n",
    "print(\"After removed duplicated word:\",len(word_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def write_list_of_words(word_new_list):\n",
    "\n",
    "    # Parent Directories \n",
    "    parent_dir = os.getcwd() \n",
    "\n",
    "\n",
    "\n",
    "    path =  parent_dir + \"\\\\\" + \"WordTokenizedFile\"\n",
    "    filePath = os.path.join(path, \"words\") \n",
    "    # Create the directory \n",
    "    isExist = os.path.exists(filePath)\n",
    "    # if not isExist:\n",
    "    file = open(filePath, \"w\" , encoding='utf-8') \n",
    "    # print(result)\n",
    "    file.write(word_new_list)\n",
    "\n",
    "    file.flush()\n",
    "    file.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_of_words(final_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c710e3be127ef2ad95d6db194e818d2faa49e132c3e6f7b047e635d15e31d42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
